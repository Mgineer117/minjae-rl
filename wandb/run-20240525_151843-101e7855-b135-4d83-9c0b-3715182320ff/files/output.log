Epoch:   0%|                                                                                                                | 0/2000 [00:00<?, ?it/s]
Training:   0%|                                                                                                               | 0/10 [00:00<?, ?it/s]
[36mSaving config:
{
    "action_dim":	"3",
    "actor_hidden_dims":	[
        256,
        256
    ],
    "agent_type":	"Hopper",
    "algo_name":	"trpo",
    "critic_lr":	0.0001,
    "data_num":	1000000,
    "device":	"cpu",
    "env_type":	"Gym",
    "episode_len":	1000,
    "episode_num":	2,
    "epoch":	2000,
    "eval_episodes":	3,
    "grad_norm":	false,
    "group":	"Gym-Hopper-seed-0",
    "hidden_dims":	[
        256,
        256
    ],
    "import_policy":	false,
    "logdir":	"log/trpo-0584-seed0/Gym-Hopper-seed-0",
    "max_action":	"1.0",
    "name":	"trpo-0584-seed0",
    "obs_shape":	[
        11
    ],
    "pklfile":	null,
    "project":	"data-collect",
    "rendering":	false,
    "seed":	0,
    "step_per_epoch":	10,
    "task":	"Gym-Hopper",
    "task_name":	null,
    "task_num":	3,
    "verbose":	true









  gym.logger.warn(
Epoch:   0%|                                                                                                     | 1/2000 [00:27<15:07:45, 27.25s/it]


















































































































































































































































































































































































Epoch:   2%|â–ˆâ–‰                                                                                                  | 38/2000 [16:37<13:38:56, 25.04s/it]























































































































Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹          | 9/10 [00:24<00:02,  2.57s/it]
data saved!!
mean reward:  1.607311440384627



















































































































































































































Epoch:   4%|â–ˆâ–ˆâ–ˆâ–Œ                                                                                                | 71/2000 [31:10<12:46:44, 23.85s/it]

































































































































































































Traceback (most recent call last):
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/collect_data/trpo_collect.py", line 149, in <module>
    train()
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/collect_data/trpo_collect.py", line 145, in train
    policy_trainer.online_train(args.seed)
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/../minjae-rl/rlkit/policy_trainer/mf_policy_trainer.py", line 138, in online_train
    loss = self.policy.learn(batch); loss['sample_time'] = sample_time
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/../minjae-rl/rlkit/policy/model_free/trpo.py", line 204, in learn
    r_pred = self.critic(embedded_obss)
  File "/home/minjae/miniconda3/envs/rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/minjae/miniconda3/envs/rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/../minjae-rl/rlkit/modules/critic_module.py", line 29, in forward
    logits = self.backbone(obs)
  File "/home/minjae/miniconda3/envs/rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/minjae/miniconda3/envs/rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/../minjae-rl/rlkit/nets/mlp.py", line 40, in forward
    return self.model(x)
  File "/home/minjae/miniconda3/envs/rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/minjae/miniconda3/envs/rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/minjae/miniconda3/envs/rl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/minjae/miniconda3/envs/rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/minjae/miniconda3/envs/rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/minjae/miniconda3/envs/rl/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt