[36mSaving config:
{
    "action_dim":	"3",
    "actor_hidden_dims":	[
        256,
        256
    ],
    "agent_type":	"Hopper",
    "algo_name":	"trpo",
    "critic_lr":	0.0001,
    "data_num":	1000000,
    "device":	"cpu",
    "env_type":	"Gym",
    "episode_len":	1000,
    "episode_num":	2,
    "epoch":	2000,
    "eval_episodes":	3,
    "grad_norm":	true,
    "group":	"Gym-Hopper-seed-0",
    "hidden_dims":	[
        256,
        256
    ],
    "import_policy":	false,
    "logdir":	"log/trpo-e771-seed0/Gym-Hopper-seed-0",
    "max_action":	"1.0",
    "name":	"trpo-e771-seed0",
    "obs_shape":	[
        11
    ],
    "pklfile":	null,
    "project":	"data-collect",
    "rendering":	false,
    "seed":	0,
    "step_per_epoch":	10,
    "task":	"Gym-Hopper",
    "task_name":	null,
    "task_num":	3,
    "verbose":	true
}
Epoch:   0%|                                                                                        | 0/2000 [00:00<?, ?it/s]









  gym.logger.warn(
Epoch:   0%|                                                                             | 1/2000 [00:20<11:24:32, 20.55s/it]




































































































































































































































































































































































































































































































Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 9/10 [00:24<00:02,  2.68s/it]
data saved!!
mean reward:  1.0087247892122349





























































































































































































































Epoch:   4%|â–ˆâ–ˆâ–‹                                                                         | 72/2000 [28:34<13:34:01, 25.33s/it]















































































































































































































Traceback (most recent call last):
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/collect_data/trpo_collect.py", line 149, in <module>
    train()
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/collect_data/trpo_collect.py", line 145, in train
    policy_trainer.online_train(args.seed)
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/../minjae-rl/rlkit/policy_trainer/mf_policy_trainer.py", line 138, in online_train
    loss = self.policy.learn(batch); loss['sample_time'] = sample_time
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/../minjae-rl/rlkit/policy/model_free/trpo.py", line 241, in learn
    stepdir = conjugate_gradients(Fvp, -loss_grad, 10, device=self.device)
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/../minjae-rl/rlkit/policy/model_free/trpo.py", line 19, in conjugate_gradients
    _Avp = Avp(p)
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/../minjae-rl/rlkit/policy/model_free/trpo.py", line 225, in Fvp_direct
    grads = torch.autograd.grad(kl, self.actor.parameters(), create_graph=True)
  File "/home/minjae/miniconda3/envs/rl/lib/python3.10/site-packages/torch/autograd/__init__.py", line 412, in grad
    result = _engine_run_backward(
  File "/home/minjae/miniconda3/envs/rl/lib/python3.10/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt