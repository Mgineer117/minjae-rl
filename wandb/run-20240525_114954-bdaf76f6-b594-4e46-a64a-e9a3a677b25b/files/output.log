[36mSaving config:
{
    "K_epochs":	5,
    "action_dim":	"1",
    "actor_hidden_dims":	[
        256,
        256
    ],
    "actor_lr":	0.0001,
    "agent_type":	"InvertedPendulum",
    "algo_name":	"ppo",
    "critic_lr":	0.001,
    "data_num":	1000000,
    "device":	"cpu",
    "env_type":	"Gym",
    "episode_len":	500,
    "episode_num":	2,
    "epoch":	100,
    "eps_clip":	0.2,
    "eval_episodes":	3,
    "group":	"Gym-InvertedPendulum-seed-0",
    "hidden_dims":	[
        256,
        256
    ],
    "import_policy":	false,
    "logdir":	"log/ppo-0494-seed0/Gym-InvertedPendulum-seed-0",
    "max_action":	"3.0",
    "name":	"ppo-0494-seed0",
    "obs_shape":	[
        4
    ],
    "project":	"osrl-test",
    "rendering":	false,
    "seed":	0,
    "step_per_epoch":	50,
    "task":	"Gym-InvertedPendulum",
    "task_name":	null,
    "task_num":	1,
    "verbose":	true
}
Epoch:   0%|                                                                               | 0/100 [00:00<?, ?it/s]
















  gym.logger.warn(
Epoch:   1%|â–‹                                                                      | 1/100 [00:35<58:13, 35.29s/it]












Traceback (most recent call last):â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                       | 33/50 [00:25<00:12,  1.36it/s]
  File "/home/minjae/miniconda3/envs/rl/lib/python3.10/logging/__init__.py", line 228, in _releaseLock
    def _releaseLock():
KeyboardInterrupt:




































































































































































































































































































































































Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 47/50 [00:35<00:02,  1.31it/s]
data saved!!
mean reward:  1.0




























































































































































































Traceback (most recent call last):
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/collect_data.py", line 153, in <module>
    train()
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/collect_data.py", line 149, in train
    policy_trainer.online_train(args.seed)
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/rlkit/policy_trainer/mf_policy_trainer.py", line 138, in online_train
    loss = self.policy.learn(batch); loss['sample_time'] = sample_time
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/rlkit/policy/model_free/ppo.py", line 187, in learn
    v_loss.backward()
  File "/home/minjae/miniconda3/envs/rl/lib/python3.10/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/home/minjae/miniconda3/envs/rl/lib/python3.10/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/home/minjae/miniconda3/envs/rl/lib/python3.10/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt