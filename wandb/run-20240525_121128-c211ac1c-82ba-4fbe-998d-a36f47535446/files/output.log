Epoch:   0%|                                                                               | 0/100 [00:00<?, ?it/s]
Training:   2%|â–ˆâ–                                                                   | 1/50 [00:01<01:12,  1.47s/it]
[36mSaving config:
{
    "K_epochs":	5,
    "action_dim":	"1",
    "actor_hidden_dims":	[
        256,
        256
    ],
    "actor_lr":	0.0001,
    "agent_type":	"InvertedPendulum",
    "algo_name":	"ppo",
    "critic_lr":	0.001,
    "data_num":	1000000,
    "device":	"cpu",
    "env_type":	"Gym",
    "episode_len":	1000,
    "episode_num":	2,
    "epoch":	100,
    "eps_clip":	0.2,
    "eval_episodes":	3,
    "group":	"Gym-InvertedPendulum-seed-0",
    "hidden_dims":	[
        256,
        256
    ],
    "import_policy":	false,
    "logdir":	"log/ppo-901d-seed0/Gym-InvertedPendulum-seed-0",
    "max_action":	"3.0",
    "name":	"ppo-901d-seed0",
    "obs_shape":	[
        4
    ],
    "project":	"osrl-test",
    "rendering":	true,
    "seed":	0,
    "step_per_epoch":	50,
    "task":	"Gym-InvertedPendulum",
    "task_name":	null,
    "task_num":	1,
    "verbose":	true



































  gym.logger.warn(
Epoch:   1%|â–‹                                                                    | 1/100 [01:12<1:59:09, 72.22s/it]





















































































































































































































































































































































Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 48/50 [01:14<00:03,  1.50s/it]
data saved!!
mean reward:  1.0
































































































































Traceback (most recent call last):
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/collect_data.py", line 153, in <module>
    train()
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/collect_data.py", line 149, in train
    policy_trainer.online_train(args.seed)
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/rlkit/policy_trainer/mf_policy_trainer.py", line 137, in online_train
    batch, sample_time = self.sampler.collect_samples(self.policy, seed)
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/rlkit/buffer/sampler.py", line 269, in collect_samples
    p.join()
  File "/home/minjae/miniconda3/envs/rl/lib/python3.10/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/home/minjae/miniconda3/envs/rl/lib/python3.10/multiprocessing/popen_fork.py", line 43, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/home/minjae/miniconda3/envs/rl/lib/python3.10/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt