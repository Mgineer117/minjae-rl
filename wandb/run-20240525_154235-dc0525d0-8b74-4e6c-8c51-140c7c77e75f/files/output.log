Epoch:   0%|                                                                                        | 0/2000 [00:00<?, ?it/s]
Training:   0%|                                                                                       | 0/10 [00:00<?, ?it/s]
[36mSaving config:
{
    "action_dim":	"3",
    "actor_hidden_dims":	[
        256,
        256
    ],
    "agent_type":	"Hopper",
    "algo_name":	"trpo",
    "critic_lr":	0.003,
    "data_num":	1000000,
    "device":	"cpu",
    "env_type":	"Gym",
    "episode_len":	1000,
    "episode_num":	2,
    "epoch":	2000,
    "eval_episodes":	3,
    "grad_norm":	true,
    "group":	"Gym-Hopper-seed-0",
    "hidden_dims":	[
        256,
        256
    ],
    "import_policy":	false,
    "logdir":	"log/trpo-7223-seed0/Gym-Hopper-seed-0",
    "max_action":	"1.0",
    "name":	"trpo-7223-seed0",
    "obs_shape":	[
        11
    ],
    "pklfile":	null,
    "project":	"data-collect",
    "rendering":	false,
    "seed":	0,
    "step_per_epoch":	10,
    "task":	"Gym-Hopper",
    "task_name":	null,
    "task_num":	3,
    "verbose":	true









  gym.logger.warn(
Epoch:   0%|                                                                             | 1/2000 [00:29<16:34:25, 29.85s/it]






































































































































































































































Epoch:   1%|â–‰                                                                           | 24/2000 [12:02<18:32:21, 33.78s/it]






































































Epoch:   2%|â–ˆâ–                                                                          | 31/2000 [15:28<16:26:44, 30.07s/it]










Epoch:   2%|â–ˆâ–                                                                          | 32/2000 [15:56<16:06:17, 29.46s/it]



















































































































































































Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 9/10 [00:29<00:03,  3.23s/it]
data saved!!
mean reward:  0.9177053943686432



















































Epoch:   3%|â–ˆâ–ˆ                                                                          | 55/2000 [27:35<15:59:47, 29.61s/it]




































































































Epoch:   3%|â–ˆâ–ˆâ–                                                                         | 65/2000 [32:34<16:20:43, 30.41s/it]


















































































































































































































































































































































































Epoch:   5%|â–ˆâ–ˆâ–ˆâ–Š                                                                       | 102/2000 [48:01<12:52:44, 24.43s/it]





















































Traceback (most recent call last):
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/collect_data/trpo_collect.py", line 149, in <module>
    train()
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/collect_data/trpo_collect.py", line 145, in train
    policy_trainer.online_train(args.seed)
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/../minjae-rl/rlkit/policy_trainer/mf_policy_trainer.py", line 138, in online_train
    loss = self.policy.learn(batch); loss['sample_time'] = sample_time
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/../minjae-rl/rlkit/policy/model_free/trpo.py", line 201, in learn
    self.critic_optim.step(closure)
  File "/home/minjae/miniconda3/envs/rl/lib/python3.10/site-packages/torch/optim/optimizer.py", line 391, in wrapper
    out = func(*args, **kwargs)
  File "/home/minjae/miniconda3/envs/rl/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/minjae/miniconda3/envs/rl/lib/python3.10/site-packages/torch/optim/lbfgs.py", line 389, in step
    al[i] = old_stps[i].dot(q) * ro[i]
KeyboardInterrupt