[36mSaving config:
{
    "action_dim":	"3",
    "actor_hidden_dims":	[
        256,
        256
    ],
    "agent_type":	"Hopper",
    "algo_name":	"trpo",
    "critic_lr":	0.003,
    "data_num":	1000000,
    "device":	"cpu",
    "env_type":	"Gym",
    "episode_len":	1000,
    "episode_num":	2,
    "epoch":	2000,
    "eval_episodes":	3,
    "group":	"Gym-Hopper-seed-0",
    "hidden_dims":	[
        256,
        256
    ],
    "import_policy":	false,
    "logdir":	"log/trpo-93d5-seed0/Gym-Hopper-seed-0",
    "max_action":	"1.0",
    "name":	"trpo-93d5-seed0",
    "obs_shape":	[
        11
    ],
    "pklfile":	null,
    "project":	"data-collect",
    "rendering":	false,
    "seed":	0,
    "step_per_epoch":	10,
    "task":	"Gym-Hopper",
    "task_name":	null,
    "task_num":	3,
    "verbose":	true
}
Epoch:   0%|                                                                                 | 0/2000 [00:00<?, ?it/s]








  gym.logger.warn(
Epoch:   0%|                                                                      | 1/2000 [00:20<11:09:44, 20.10s/it]


































































































































































































































































































































Traceback (most recent call last):
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/collect_data/trpo_collect.py", line 147, in <module>
    train()
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/collect_data/trpo_collect.py", line 143, in train
    policy_trainer.online_train(args.seed)
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/../minjae-rl/rlkit/policy_trainer/mf_policy_trainer.py", line 138, in online_train
    loss = self.policy.learn(batch); loss['sample_time'] = sample_time
  File "/home/minjae/Documents/Minjae/Research/minjae-rl/../minjae-rl/rlkit/policy/model_free/trpo.py", line 199, in learn
    self.critic_optim.step(closure)
  File "/home/minjae/miniconda3/envs/rl/lib/python3.10/site-packages/torch/optim/optimizer.py", line 391, in wrapper
    out = func(*args, **kwargs)
  File "/home/minjae/miniconda3/envs/rl/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/minjae/miniconda3/envs/rl/lib/python3.10/site-packages/torch/optim/lbfgs.py", line 389, in step
    al[i] = old_stps[i].dot(q) * ro[i]
KeyboardInterrupt